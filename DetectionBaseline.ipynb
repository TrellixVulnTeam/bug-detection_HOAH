{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651cda0a",
   "metadata": {},
   "source": [
    "# Bug Detection\n",
    "\n",
    "In this notebook I will show the baseline model for the bug detection project. This will be the simplest model that can be created to try and solve the problem of bug detection. The main idea of this model is that for each token of code we can predict if it is buggy or not. However we will be using multi class labels where each bug is an actual error class.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8099b26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T17:21:04.542151Z",
     "start_time": "2022-02-07T17:21:03.130749Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import codenet\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "models_path = \"../models/\"\n",
    "\n",
    "vectorizer_path = models_path + \"vectorizer_fasttext.model\"\n",
    "classifier_path = models_path + \"classifier_decisiontree.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d6868",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "In this section we load the data generated from the CodeNet subset we explored previously. The training data will consist of tokens of code as input and the respective error class as target. From the data exploration section we know that an error can be caused by a sequence of tokens, or an instruction. To label each token we will create an array of labels, each corresponding to a token. Then for each error class, if it's range includes a token we mark it with that label. In the end, the data will be a sequence of token, error class pairs. We want our model to be able to learn which token corresponds to which error class. The limitation of this method is the fact that we will not consider the context of the problem. We will consider each token to be independent of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e0459ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T17:21:20.707571Z",
     "start_time": "2022-02-07T17:21:17.837024Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_X_y(X, y):\n",
    "    target_y = []\n",
    "    \n",
    "    for x, errs in zip(X, y):\n",
    "        new_y = np.empty_like(x)\n",
    "        new_y.fill(\"Accepted\")\n",
    "        for [i1, i2, err] in errs:\n",
    "            new_y[i1:max(i1+1, i2)] = err\n",
    "        target_y.append(new_y)\n",
    "        \n",
    "    return X, target_y\n",
    "\n",
    "with open(codenet.detection_X_y_v2_path, 'rb') as f:\n",
    "    X, ys = pickle.load(f)\n",
    "    X, y = convert_X_y(X, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472480e5",
   "metadata": {},
   "source": [
    "Here we split the data into training and testing subsets with 80% of the sample for training and the rest for testing. Then we use FastText to vectorize the tokens into float arrays with size 16. I chose FastText because it handles words that are also out of vocabulary, like for example variable names that can be unique to a specific submission. Then we transform the input data from a sequence of tokens to an array containing the vectors for each token. The resulting data will be a sequence of vector, label pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bbda7b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T17:24:36.133372Z",
     "start_time": "2022-02-07T17:24:36.052075Z"
    }
   },
   "outputs": [],
   "source": [
    "src_train, src_test, ys_train, ys_test = train_test_split(X, ys, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c284163",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T17:25:32.971443Z",
     "start_time": "2022-02-07T17:25:32.553664Z"
    }
   },
   "outputs": [],
   "source": [
    "src_train, yss_train = convert_X_y(src_train, ys_train)\n",
    "src_test, yss_test = convert_X_y(src_test, ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1ba9a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T13:51:16.554450Z",
     "start_time": "2022-01-22T13:50:10.484438Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [x.tolist() for x in src_train]\n",
    "\n",
    "vectorizer = FastText(sentences=sentences, min_count=0, vector_size=16, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f21d8a9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T15:41:38.267025Z",
     "start_time": "2022-01-22T15:41:38.098012Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer.save(vectorizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "343f77ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T10:32:39.767792Z",
     "start_time": "2022-01-28T10:31:40.489234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of accepted tokens in the test set 0.8736134751773049\n"
     ]
    }
   ],
   "source": [
    "tok_train = np.concatenate(src_train)\n",
    "tok_test = np.concatenate(src_test)\n",
    "\n",
    "X_train = vectorizer.wv[tok_train]\n",
    "X_test = vectorizer.wv[tok_test]\n",
    "y_train = np.concatenate(yss_train)\n",
    "y_test = np.concatenate(yss_test)\n",
    "\n",
    "print(f\"Percent of accepted tokens in the test set {np.sum(y_test == 'Accepted') / len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc239758",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The baseline classifier we are going to use is a decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87fccdb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T13:51:57.599764Z",
     "start_time": "2022-01-22T13:51:57.595340Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2cf9013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T13:55:28.908114Z",
     "start_time": "2022-01-22T13:51:57.601257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf9a7b14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T16:28:39.645471Z",
     "start_time": "2022-01-27T16:28:37.130811Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3d55fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T16:30:35.141747Z",
     "start_time": "2022-01-27T16:28:41.002615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8728657548125633\n",
      "Precision 0.787925806136263\n",
      "Recall 0.8728657548125633\n",
      "F1 0.8151843545926623\n"
     ]
    }
   ],
   "source": [
    "print(\"Token Level Prediction\")\n",
    "print(f\"Accuracy {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision {precision_score(y_test, y_pred, average='weighted', zero_division=0)}\")\n",
    "print(f\"Recall {recall_score(y_test, y_pred, average='weighted', zero_division=0)}\")\n",
    "print(f\"F1 {f1_score(y_test, y_pred, average='weighted', zero_division=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f2f2ee7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T10:11:47.411956Z",
     "start_time": "2022-01-28T10:11:35.359675Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19508/19508 [00:11<00:00, 1627.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Level Prediction: Accepted vs non Accepted\n",
      "Accuracy 0.515173262251384\n",
      "Precision 0.5086231430360294\n",
      "Recall 0.9154799713144145\n",
      "F1 0.6539334065129894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loop = tqdm(list(zip(src_test, yss_test)))\n",
    "\n",
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "tn = 0\n",
    "\n",
    "for i, (x_src, y_ref) in enumerate(loop):\n",
    "    x_src = vectorizer.wv[x_src]\n",
    "    y_p = clf.predict(x_src)\n",
    "    \n",
    "    ref_accepted = np.all(y_ref == \"Accepted\")\n",
    "    p_accepted = np.all(y_p == \"Accepted\")\n",
    "    \n",
    "    if ref_accepted and p_accepted:\n",
    "        tp += 1\n",
    "    if not ref_accepted and p_accepted:\n",
    "        fp += 1\n",
    "    if ref_accepted and not p_accepted:\n",
    "        fn += 1\n",
    "    if not ref_accepted and not p_accepted:\n",
    "        tn += 1\n",
    "\n",
    "print(\"Document Level Prediction: Accepted vs non Accepted\")\n",
    "print(f\"Accuracy {(tp + tn) / (tp + fp + fn + tn)}\")\n",
    "print(f\"Precision {tp / (tp + fp)}\")\n",
    "print(f\"Recall {tp / (tp + fn)}\")\n",
    "print(f\"F1 {(2 * tp) / (2 * tp + fp + fn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6112e1a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T17:34:45.879102Z",
     "start_time": "2022-02-07T17:34:33.459912Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████| 19508/19508 [00:12<00:00, 1621.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement Level Prediction: Accepted vs non Accepted\n",
      "Accuracy 0.6681046977576947\n",
      "Precision 0.9797950262430737\n",
      "Recall 0.6409246100437057\n",
      "F1 0.7749332223262914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loop = tqdm(list(zip(src_test, ys_test)))\n",
    "\n",
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "tn = 0\n",
    "\n",
    "for i, (x_src, y_ref) in enumerate(loop):\n",
    "    x_src = vectorizer.wv[x_src]\n",
    "    y_p = clf.predict(x_src)\n",
    "    \n",
    "    start_i = 0\n",
    "    for i1, i2, err in y_ref:\n",
    "        if np.all(y_p[start_i:i1] == \"Accepted\"):\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "        \n",
    "        if np.all(y_p[i1:i2] == err):\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "            \n",
    "        start_i = i2\n",
    "    \n",
    "    if y_p.shape[0] > start_i:\n",
    "        if np.all(y_p[start_i:] == \"Accepted\"):\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "print(\"Statement Level Prediction: Accepted vs non Accepted\")\n",
    "print(f\"Accuracy {(tp + tn) / (tp + fp + fn + tn)}\")\n",
    "print(f\"Precision {tp / (tp + fp)}\")\n",
    "print(f\"Recall {tp / (tp + fn)}\")\n",
    "print(f\"F1 {(2 * tp) / (2 * tp + fp + fn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "813fe69e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T15:46:00.487209Z",
     "start_time": "2022-01-22T15:46:00.435671Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(classifier_path, \"wb\") as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3d65c5",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "To perform inference we will take as input the Python source code, or the source file path, and tokenize it. We will thus obtain a sequence of all the tokens, their sequence number, line and column. Then we take the tokens and use our vectorizer to extract the features and prepare the data for inference. Finally we can run the model, the decision tree baseline, and obtain the list of errors for each token in our source code. We filter out the tokens without errors and only show the ones which have associated errors to them. To check the model we can also run the source code file and see what output we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f2915f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T17:22:26.217234Z",
     "start_time": "2022-02-07T17:22:16.606917Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = FastText.load(vectorizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4251fac8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T17:22:26.283028Z",
     "start_time": "2022-02-07T17:22:26.218641Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(classifier_path, \"rb\") as f:\n",
    "    clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97c7cc63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T16:51:27.010672Z",
     "start_time": "2022-01-27T16:51:27.001501Z"
    }
   },
   "outputs": [],
   "source": [
    "def inferbug_python(source_code):\n",
    "    token_df = codenet.run_pythontokenizer_str(source_code)\n",
    "    tokens = token_df[\"text\"].values\n",
    "    tokens = vectorizer.wv[tokens]\n",
    "\n",
    "    prediction = clf.predict(tokens)\n",
    "    token_df[\"prediction\"] = prediction\n",
    "    token_err_df = token_df[token_df[\"prediction\"] != \"Accepted\"]\n",
    "    \n",
    "    return token_err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0010edac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T16:51:27.955171Z",
     "start_time": "2022-01-27T16:51:27.462738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seqnr</th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>channel</th>\n",
       "      <th>line</th>\n",
       "      <th>column</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>double</td>\n",
       "      <td>name</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>SyntaxError</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seqnr  start  stop    text class  channel  line  column   prediction\n",
       "14     14     19    24  double  name        0     1      19  SyntaxError"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(14, 14, 'SyntaxError')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('',\n",
       " 'Traceback (most recent call last):\\n  File \"<string>\", line 1, in <module>\\nNameError: name \\'double\\' is not defined\\n',\n",
       " 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug = \"\"\"mylist = [1, 2, 3, double]\n",
    "\n",
    "print(mylist[123])\n",
    "\"\"\"\n",
    "\n",
    "token_err_df = inferbug_python(bug)\n",
    "\n",
    "display(token_err_df)\n",
    "display(err_df2list(token_err_df))\n",
    "codenet.exec_python_str(bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d4cf62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
