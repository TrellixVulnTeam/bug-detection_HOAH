{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89ceaf8e",
   "metadata": {},
   "source": [
    "# Bug Detection\n",
    "\n",
    "In this notebook I will show the baseline model for the bug detection project. This will be the simplest model that can be created to try and solve the problem of bug detection. The main idea of this model is that for each token of code we can predict if it is buggy or not. However we will be using multi class labels where each bug is an actual error class.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f99bad5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T12:43:16.834759Z",
     "start_time": "2022-01-22T12:43:15.595019Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import codenet\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d362d31d",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "In this section we load the data generated from the CodeNet subset we explored previously. The training data will consist of tokens of code as input and the respective error class as target. From the data exploration section we know that an error can be caused by a sequence of tokens, or an instruction. To label each token we will create an array of labels, each corresponding to a token. Then for each error class, if it's range includes a token we mark it with that label. In the end, the data will be a sequence of token, error class pairs. We want our model to be able to learn which token corresponds to which error class. The limitation of this method is the fact that we will not consider the context of the problem. We will consider each token to be independent of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "128a1a73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T12:43:20.369151Z",
     "start_time": "2022-01-22T12:43:16.836353Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_X_y(X, y):\n",
    "    target_y = []\n",
    "    \n",
    "    for x, errs in zip(X, y):\n",
    "        new_y = np.empty_like(x)\n",
    "        new_y.fill(\"Accepted\")\n",
    "        for [i1, i2, err] in errs:\n",
    "            new_y[i1:max(i1+1, i2)] = err\n",
    "        target_y.append(new_y)\n",
    "        \n",
    "    return X, target_y\n",
    "\n",
    "with open(codenet.detection_X_y_v2_path, 'rb') as f:\n",
    "    X, y = pickle.load(f)\n",
    "    X, y = convert_X_y(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449c5135",
   "metadata": {},
   "source": [
    "Here we split the data into training and testing subsets with 80% of the sample for training and the rest for testing. Then we use FastText to vectorize the tokens into float arrays with size 16. I chose FastText because it handles words that are also out of vocabulary, like for example variable names that can be unique to a specific submission. Then we transform the input data from a sequence of tokens to an array containing the vectors for each token. The resulting data will be a sequence of vector, label pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e85d38c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T13:19:23.130266Z",
     "start_time": "2022-01-22T13:19:23.047958Z"
    }
   },
   "outputs": [],
   "source": [
    "tok_train, tok_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29c9cd89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T12:44:29.755469Z",
     "start_time": "2022-01-22T12:43:20.485037Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [x.tolist() for x in tok_train]\n",
    "\n",
    "vectorizer = FastText(sentences=sentences, min_count=0, vector_size=16, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bec3c383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T12:45:12.175266Z",
     "start_time": "2022-01-22T12:44:29.757811Z"
    }
   },
   "outputs": [],
   "source": [
    "tok_train = np.concatenate(tok_train)\n",
    "tok_test = np.concatenate(tok_test)\n",
    "\n",
    "X_train = vectorizer.wv[tok_train]\n",
    "X_test = vectorizer.wv[tok_test]\n",
    "y_train = np.concatenate(y_train)\n",
    "y_test = np.concatenate(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6452fc",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The baseline classifier we are going to use is a decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47b04f02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T12:45:56.446316Z",
     "start_time": "2022-01-22T12:45:56.439408Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db11a0c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T12:49:57.988729Z",
     "start_time": "2022-01-22T12:46:02.445561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "608b0634",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T12:49:58.964862Z",
     "start_time": "2022-01-22T12:49:57.990623Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58964951",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T12:51:34.611536Z",
     "start_time": "2022-01-22T12:49:58.966128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8728067375886525\n",
      "Precision 0.7871550503899936\n",
      "Recall 0.8728067375886525\n",
      "F1 0.8151476436521569\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision {precision_score(y_test, y_pred, average='weighted', zero_division=0)}\")\n",
    "print(f\"Recall {recall_score(y_test, y_pred, average='weighted', zero_division=0)}\")\n",
    "print(f\"F1 {f1_score(y_test, y_pred, average='weighted', zero_division=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc5ae3",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "69969094",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T13:23:39.352797Z",
     "start_time": "2022-01-22T13:23:39.091718Z"
    }
   },
   "outputs": [],
   "source": [
    "bug = \"\"\"mylist = [1, 2, 3, double]\n",
    "\n",
    "print(mylist[123])\n",
    "\"\"\"\n",
    "\n",
    "token_df = codenet.run_pythontokenizer_str(bug)\n",
    "tokens = token_df[\"text\"].values\n",
    "tokens = vectorizer.wv[tokens]\n",
    "\n",
    "prediction = clf.predict(tokens)\n",
    "token_err_df = codenet.prediction2err(token_df, prediction)\n",
    "\n",
    "display(token_err_df)\n",
    "codenet.exec_python_str(bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175aa36c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
